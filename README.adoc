image:https://img.shields.io/badge/License-PMPL--1.0-blue.svg[License: PMPL-1.0,link="https://github.com/hyperpolymath/palimpsest-license"]
image:https://img.shields.io/badge/Philosophy-Palimpsest-indigo.svg[Palimpsest,link="https://github.com/hyperpolymath/palimpsest-license"]


// SPDX-License-Identifier: PMPL-1.0-or-later
// SPDX-FileCopyrightText: 2025 Jonathan D.A. Jewell
= NeuroPhone - Neurosymbolic AI Android Application
:toc:
:toclevels: 3

image:https://img.shields.io/badge/Platform-Android-green[Platform]
image:https://img.shields.io/badge/Device-Oppo%20Reno%2013-blue[Target Device]
image:https://img.shields.io/badge/RSR-Bronze-cd7f32[RSR Compliance]

== What This Is

**neurophone** is a complete *Android application* for neurosymbolic AI on mobile devices. It combines spiking neural networks with large language models for on-device intelligence.

IMPORTANT: This is an *application*, NOT a library. For the underlying platform-agnostic routing library, see link:https://github.com/hyperpolymath/heterogenous-mobile-computing[mobile-ai-orchestrator].

== Target Device

Primary target: **Oppo Reno 13** (MediaTek Dimensity 8350)

* 12GB RAM
* NPU acceleration available
* Android 14+

Also compatible with Android 8.0+ devices with 4GB+ RAM.

== Core Purpose

```
┌─────────────────────────────────────────────────────────────────┐
│                      NEUROPHONE                                 │
│                   (THIS APPLICATION)                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │
│  │  Sensors    │─────▶│    LSM      │─────▶│   Bridge    │     │
│  │ Accel/Gyro  │      │  (spiking   │      │  (state     │     │
│  │ Light/Prox  │      │  reservoir) │      │  encoding)  │     │
│  └─────────────┘      └─────────────┘      └──────┬──────┘     │
│                                                   │            │
│                                                   ▼            │
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │
│  │   Output    │◀─────│    ESN      │◀────▶│    LLM      │     │
│  │  (actions)  │      │  (echo      │      │ (Llama 3.2) │     │
│  │             │      │  reservoir) │      │             │     │
│  └─────────────┘      └─────────────┘      └─────────────┘     │
│                                                                 │
│  Processes: Sensor data → Neural interpretation → LLM query    │
│  Runs: ON THE DEVICE, with cloud fallback                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼ (cloud fallback)
                    ┌─────────────────────┐
                    │      Claude API     │
                    │  (complex queries)  │
                    └─────────────────────┘
```

== Key Differentiators

|===
| Feature | This App | Typical Mobile AI Apps

| *Neural Processing*
| On-device LSM + ESN (spiking networks)
| Cloud-only or simple TFLite

| *Sensor Integration*
| Real-time sensor → neural → LLM pipeline
| Separate sensor and AI components

| *LLM*
| Local Llama 3.2 + Claude fallback
| Cloud-only

| *Latency*
| <100ms local inference
| 500ms+ network round-trip

| *Privacy*
| Sensor data stays on device
| Often sent to cloud
|===

== Architecture

=== Rust Crates (8 modules)

|===
| Crate | Purpose | Key Features

| `lsm`
| Liquid State Machine
| 512 spiking neurons, 3D grid, 1kHz processing

| `esn`
| Echo State Network
| 300-neuron reservoir, ridge regression

| `bridge`
| Neural ↔ Symbolic
| State encoding, context generation

| `sensors`
| Phone Sensors
| Accel, gyro, magnetometer, light, proximity

| `llm`
| Local Inference
| Llama 3.2 via llama.cpp, streaming

| `claude-client`
| Cloud Fallback
| Claude API, retry logic, context injection

| `neurophone-core`
| Orchestration
| Main coordinator, query routing

| `neurophone-android`
| Android JNI
| Kotlin ↔ Rust bridge
|===

=== Android App (Kotlin)

```
android/
├── app/src/main/
│   ├── java/ai/neurophone/
│   │   ├── MainActivity.kt
│   │   ├── NativeLib.kt          # JNI interface
│   │   ├── SensorManager.kt      # Sensor collection
│   │   └── ui/                   # Compose UI
│   └── res/
└── build.gradle.kts
```

== Components

=== LSM (Liquid State Machine)

Spiking neural network for temporal sensor processing:

* 3D grid: 8×8×8 = 512 Leaky Integrate-and-Fire neurons
* Distance-dependent connectivity
* Excitatory/inhibitory balance
* Real-time spike processing at 1kHz

=== ESN (Echo State Network)

Reservoir for state prediction:

* 300-neuron reservoir
* Spectral radius: 0.95
* Leaky integrator dynamics
* Ridge regression output

=== Sensors

Phone sensor integration:

* Accelerometer, gyroscope, magnetometer
* Light and proximity sensors
* IIR filtering (low-pass, high-pass)
* Feature extraction at 50Hz

=== Bridge

Neural ↔ Symbolic translation:

* Integrates LSM + ESN states
* Generates natural language context for LLMs
* Temporal pattern detection
* Salience and urgency computation

=== Local LLM

On-device language model:

* Llama 3.2 1B/3B via llama.cpp
* Optimized for Dimensity 8350
* Q4_K_M quantization (~700MB)
* Neural context injection

=== Claude Client

Cloud fallback for complex queries:

* Messages API integration
* Automatic retry with exponential backoff
* Hybrid inference (local/cloud decision)
* Neural state context injection

== Getting Started

=== Prerequisites

* Rust 1.75+
* Android NDK 26+
* Android Studio (for app development)
* Oppo Reno 13 or Android 8.0+ device

=== Build

```bash
# Clone
git clone https://github.com/hyperpolymath/neurophone
cd neurophone

# Setup
./scripts/setup.sh

# Build native libraries for Android
./scripts/build-android.sh

# Open android/ in Android Studio
```

=== Download LLM Model

```bash
# Download Llama 3.2 1B Instruct (Q4_K_M, ~700MB)
# From: https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF

# Push to device
adb push llama-3.2-1b-instruct-q4_k_m.gguf /data/local/tmp/
```

=== Configure

Set Claude API key (for cloud fallback):

```bash
export ANTHROPIC_API_KEY="your-api-key"
```

Or in `config/default.toml`:

```toml
[claude]
api_key = "your-api-key"
model = "claude-sonnet-4-20250514"

[llm]
model_path = "/data/local/tmp/llama-3.2-1b-q4_k_m.gguf"
n_threads = 4
context_size = 2048
```

== Usage

=== Kotlin API

```kotlin
// Initialize
NativeLib.init()
NativeLib.start()

// Query with neural context
val response = NativeLib.query("What's my current activity?", preferLocal = true)

// Get raw neural state
val context = NativeLib.getNeuralContext()
// Returns: [NEURAL_STATE] Description: ... [/NEURAL_STATE]

// Cleanup
NativeLib.stop()
```

=== Rust API

```rust
use neurophone_core::{NeuroSymbolicSystem, SystemConfig};

let mut system = NeuroSymbolicSystem::with_config(config)?;
let _rx = system.start().await?;

// Send sensor data
system.send_sensor(reading).await?;

// Query
let response = system.query("What's happening?", true).await?;

// Get neural context
let context = system.get_neural_context().await;
```

== Performance

Optimized for Oppo Reno 13 (Dimensity 8350):

|===
| Component | Latency | Notes

| Sensor processing
| <1ms
| 50Hz loop

| LSM step
| <2ms
| 512 neurons

| ESN step
| <1ms
| 300 neurons

| Bridge integration
| <1ms
| Per step

| Local LLM (1B)
| 50-100ms/token
| Q4 quantized

| Claude API
| 500-2000ms
| Network dependent
|===

== Relationship to mobile-ai-orchestrator

This application and link:https://github.com/hyperpolymath/heterogenous-mobile-computing[mobile-ai-orchestrator] are complementary:

|===
| | neurophone | mobile-ai-orchestrator

| *Type*
| Application
| Library

| *Platform*
| Android-specific
| Platform-agnostic

| *Focus*
| Sensor → Neural → LLM pipeline
| AI routing decisions

| *Neural*
| LSM, ESN (spiking networks)
| MLP, Reservoir (routing)

| *Use Case*
| Run on phone, process sensors
| Embed in any app for routing
|===

**Future integration**: neurophone may adopt mobile-ai-orchestrator for its routing decisions, combining:

* neurophone's sensor processing + neural interpretation
* mobile-ai-orchestrator's intelligent local/cloud routing

== Related Projects

|===
| Project | Relationship | Description

| link:https://github.com/hyperpolymath/heterogenous-mobile-computing[*mobile-ai-orchestrator*]
| *Complementary library*
| Platform-agnostic AI routing (may integrate)

| link:https://github.com/hyperpolymath/echomesh[echomesh]
| Related
| Conversation context preservation

| link:https://github.com/hyperpolymath/oblibeny[oblibeny]
| Related
| Safety-critical programming concepts
|===

== RSR Compliance

Bronze-level RSR (Rhodium Standard Repository) compliance:

* Type safety (Rust)
* Memory safety (ownership model)
* Comprehensive documentation
* Build automation
* Security policy

== Development

```bash
# Run tests
cargo test

# Build for Android
./scripts/build-android.sh

# Generate docs
cargo doc --open
```

== Contributing

Contributions welcome! See link:CONTRIBUTING.md[CONTRIBUTING.md].

== License

Palimpsest-MPL-1.0 License - See LICENSE file

== Citation

```bibtex
@software{neurophone_2025,
  author = {Jewell, Jonathan D.A.},
  title = {NeuroPhone: Neurosymbolic AI Android Application},
  year = {2025},
  url = {https://github.com/hyperpolymath/neurophone},
  note = {On-device LSM + ESN + LLM}
}
```

== Contact

* *Author*: Jonathan D.A. Jewell
* *Email*: hyperpolymath@protonmail.com

---

*Android Application • On-Device Neural Processing • Spiking Networks • Local LLM*
